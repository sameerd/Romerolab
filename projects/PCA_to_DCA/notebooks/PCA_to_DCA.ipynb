{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Principal Component (PCA) to Direct Coupling Analysis (DCA) of Coevolution in Proteins\n",
    "\n",
    "This notebook takes a look at a 2013 paper from Simona Cocco, Remi Monasson, Martin Weigt titled\n",
    "**From Principal Component to Direct Coupling Analysis of Coevolution in Proteins: Low-Eigenvalue Modes are Needed for Structure Prediction.** *\\[2013Cocco\\]*\n",
    "\n",
    "Link: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003176\n",
    "\n",
    "This paper looks at extracting functional and structural information from Multiple Sequence Alignments (MSA) of homologous proteins. First a covariance matrix of the residues are created from the MSA. Then the paper connects two approaches \n",
    "\n",
    "*  PCA - which identifies correlated groups of residues\n",
    "*  DCA - which identifies residue-residue contacts\n",
    "\n",
    "It shows how these two methods are related in non-intuitive ways using sophisticated statistical-physics models. This connection between the two approaches allows one to perform some sort of \"dimension reduction\" on DCA and to accurately predict residue-residue contacts with a smaller number of parameters. It also shows that the low eigenvalue values, which are discarded by PCA, are actually important to recover contact information. \n",
    "\n",
    "### Sections\n",
    "\n",
    "1. [Multiple Sequence Alignment](#msa)\n",
    "2. [Re-weighting Sequences](#reweight)\n",
    "3. [Compute Single and Double Site marginals](#marginals)\n",
    "4. [Compute the Covariance matrix](#covmat)\n",
    "5. [Maximum Entropy Modeling and Direct Coupling Analysis](#maxent)\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Sequence Alignment (MSA)  <a id=\"msa\"></a>\n",
    "\n",
    "We use a [DHFR](https://www.uniprot.org/uniprot/P00374) alignment as an example to try out the methods of the papers. The alignment file is generated by [jackhmmer](https://www.ebi.ac.uk/Tools/hmmer/search/jackhmmer)\n",
    "\n",
    "\n",
    "```\n",
    "sp|P00374|DYR_HUMAN Dihydrofolate reductase OS=Homo sapiens OX=9606 GN=DHFR PE=1 SV=2\n",
    "MVGSLNCIVAVSQNMGIGKNGDLPWPPLRNEFRYFQRMTTTSSVEGKQNLVIMGKKTWFS\n",
    "IPEKNRPLKGRINLVLSRELKEPPQGAHFLSRSLDDALKLTEQPELANKVDMVWIVGGSS\n",
    "VYKEAMNHPGHLKLFVTRIMQDFESDTFFPEIDLEKYKLLPEYPGVLSDVQEEKGIKYKF\n",
    "EVYEKND\n",
    "```\n",
    "\n",
    "Note: Not exactly sure how the alignment is generated. \n",
    "\n",
    "```shell\n",
    "jackhmmer -A DHFR_uniref90.aln --noali --notextw P00374.fasta uniref90.fasta\n",
    "# some filtering script\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape = (56165, 186) ,dtype=  |S1\n"
     ]
    }
   ],
   "source": [
    "datadir = \"../data\"\n",
    "msa_file = os.path.join(datadir, \"DHFR.aln\")\n",
    "\n",
    "# Read all the lines in the file into a 2D array of type S1\n",
    "with open(msa_file) as fh:\n",
    "    arr = np.array([[x for x in line.strip()] for line in fh], np.dtype(\"S1\"))\n",
    "\n",
    "print(\"shape =\", arr.shape, \",dtype= \", arr.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences : 56165 \n",
      "Sequence Length : 186\n"
     ]
    }
   ],
   "source": [
    "# M is the number of sequences\n",
    "# L is the length\n",
    "M, L = arr.shape\n",
    "print(\"Number of sequences : {} \".format(M))\n",
    "print(\"Sequence Length : {}\".format(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'VRPLNCIVAVSQNMGIGKNGDLPWPPLRNEFKYFQRMTTTSSVEGKQNLVIMGRKTWFSIPEKNRPLKDRINIVLSRELKEPPRGAHFLAKSLDDALRLIEQPELASKVDMVWIVGGSSVYQEAMNQPGHLRLFVTRIMQEFESDTFFPEIDLGKYKLLPEYPGVLSEVQEEKGIKYKFEVYEKKD'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first sequence\n",
    "arr[0, :].tostring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'----SIVVVMCKRFGIGRNGVLPWSPLQADMQRFRSITAG-------GGVIMGRTTFDSIPEEHRPLQGRLNVVLTTSADLMKNSNIIFVSSFDELDAIVGL----HDHLPWHVIGGVSVYQHFLEKSQVTSMYVTFVDGSLECDTFFPHQFLSHFEITRA---SALMSDTTSGMSYRFVDYTR--'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the second sequence\n",
    "arr[1, :].tostring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can order the amino acids any way we like. Here is a sorting based on some amino acid properties. \n",
    "https://proteinstructures.com/Structure/Structure/amino-acids.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMINO_ACIDS = np.array([aa for aa in \"RKDEQNHSTCYWAILMFVPG-\"], \"S1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the weights of each sequence <a id=\"reweight\"></a>\n",
    "\n",
    "To compute the weight of a sequence, we first compute the hamming distance between this sequence and all the other sequences in the alignment. Then we count the number of these distances that are less than a cutoff. \n",
    "\n",
    "This count is 1 for isolated sequences. It is large for sequences that have many similar sequences in the alignment. The weight is reciprocal of the count. So it is 1 for isolated sequences and close to zero for sequences that have many similar sequences in the MSA. (Eqn 27 in 2013Cocco)\n",
    "\n",
    "$$w_m = \\frac{1}{ \\| \\{ n | 1 \\leq n \\leq M  ; d_H [ (a_1^n, \\ldots, a_L^n), (a_1^m, \\ldots, a_L^m) ] \\leq xL \\} \\| }$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005681818181818182"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamming_cutoff = 0.2 # This is x in the equation above\n",
    "def compute_weight(index, x=hamming_cutoff, arr=arr):\n",
    "    hamming_distances = np.sum(arr[index, :] != arr, axis=1)\n",
    "    count = np.sum(hamming_distances <= x * arr.shape[1]) # L = arr.shape[1]\n",
    "    return (1.0 / count)\n",
    "\n",
    "# compute the weight of the first sequence\n",
    "compute_weight(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from :  ../data/DHFR.weights.npy\n"
     ]
    }
   ],
   "source": [
    "progress_bar = True\n",
    "try:\n",
    "    from IPython.display import clear_output\n",
    "except ImportError:\n",
    "     progress_bar = False\n",
    "\n",
    "weights_file = os.path.join(datadir, \"DHFR.weights.npy\")\n",
    "\n",
    "if os.path.isfile(weights_file):\n",
    "    weights = np.load(weights_file)\n",
    "    print(\"Loading weights from : \", weights_file)\n",
    "\n",
    "else:\n",
    "    weights = np.zeros(M)\n",
    "\n",
    "    for i in range(M):\n",
    "        weights[i] = compute_weight(i)\n",
    "        if i % 100 == 0:\n",
    "            if progress_bar:\n",
    "                clear_output(wait=True)\n",
    "            print (\"Processing sequence\", i, \"of\", M)\n",
    "    np.save(weights_file, weights)\n",
    "    print(\"Finished computing sequence weights and saved to : \", weights_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15238\n"
     ]
    }
   ],
   "source": [
    "# number of effective sequences\n",
    "M_eff = sum(weights) # Eqn 28 in 2013Cocco\n",
    "print(int(round(M_eff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q is the alphabet\n",
    "q = len(AMINO_ACIDS)\n",
    "pseudo_count = 10 * round(M_eff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Weighted Single and Double site marginals <a id=\"marginals\"></a>\n",
    "\n",
    "We first compute the weighted counts for the single site marginals $$\\sum_{m=1}^M w_m \\delta_{a, a_i^m},$$ and then we compute the weighted counts for the double site marginals $$\\sum_{m=1}^M w_m \\delta_{a, a_i^m} \\delta_{b, a_j^m}.$$ The second computation takes a long time since the arrays are too large to compute outer products and broadcast them. \n",
    "\n",
    "To get the actual marignals we need to divide these weighted counts above by the sum of the weights. However, we only do this after adding a pseudocount to regularize the resulting covariance matrix. (Eqns 29 and 30 in 2013Cocco.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading single site marginals from  ../data/DHFR.single.npy\n",
      "Loading double site marginals from  ../data/DHFR.double.npy\n"
     ]
    }
   ],
   "source": [
    "single_site_marginal_file = os.path.join(datadir, \"DHFR.single.npy\")\n",
    "double_site_marginal_file = os.path.join(datadir, \"DHFR.double.npy\")\n",
    "\n",
    "if os.path.isfile(double_site_marginal_file) and os.path.isfile(single_site_marginal_file):\n",
    "    f_i_a = np.load(single_site_marginal_file)\n",
    "    print(\"Loading single site marginals from \", single_site_marginal_file)\n",
    "\n",
    "    f_i_j_a_b = np.load(double_site_marginal_file)\n",
    "    print(\"Loading double site marginals from \", double_site_marginal_file)    \n",
    "\n",
    "else:\n",
    "    # We first compute a one-hot matrix of shape (M, L, q)\n",
    "    # which is 0/1 in index (m, i, a) depending on whether \n",
    "    # Protein *m* has amino acid *a* in position *i* \n",
    "    arr_onehot = np.zeros(arr.shape + (q,), dtype=np.uint8)\n",
    "    for i, a in enumerate(AMINO_ACIDS):\n",
    "        arr_onehot[..., i] = (arr == a)\n",
    "    print(\"arr_onehot.shape = {}\".format(arr_onehot.shape))\n",
    "    \n",
    "    # we reorder the one-hot axes so that the sequences are in the last dimension\n",
    "    # this allows us to multiply easily by the weights using broadcasting\n",
    "    arr_onehot_reorder = np.moveaxis(arr_onehot, 0, 2)\n",
    "    weighted_arr_onehot = arr_onehot_reorder * weights\n",
    "\n",
    "    # Weighted Single Site Marignals\n",
    "    f_i_a = np.sum((arr_onehot_reorder * weights), axis=-1)\n",
    "    print(\"f_i_a.shape = {}\".format(f_i_a.shape)) \n",
    "    \n",
    "    # Set up the weighted double site marginals array\n",
    "    f_i_j_a_b = np.zeros((L, q, L, q), dtype=f_i_a.dtype)\n",
    "    \n",
    "    # we cannot use outer products here because our arrays are too big\n",
    "    # So we iterate\n",
    "    for j, b in itertools.product(range(L), range(q)):\n",
    "        f_i_j_a_b[:, :, j, b] = np.sum((weighted_arr_onehot * arr_onehot_reorder[j, b, :]), axis=-1)\n",
    "    if progress_bar:\n",
    "        clear_output(wait=True)\n",
    "    print(\"Finished processing j={}, b={}, AA={}\".format(j, b, AMINO_ACIDS[b].tostring().decode()))\n",
    "\n",
    "    np.save(single_site_marginal_file, f_i_a)\n",
    "    np.save(double_site_marginal_file, f_i_j_a_b)\n",
    "    \n",
    "    # delete large temporary arrays\n",
    "    del weighted_arr_onehot, arr_onehot_reorder, arr_onehot\n",
    "    print(\"Finished computing single and double site marginals and saved to cache files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Pseudo count and compute the marginals (Eqn 29 and 30 2013Cocco)\n",
    "\n",
    "f_i_a = (1. / (M_eff + pseudo_count)) * (pseudo_count/q + f_i_a)\n",
    "f_i_j_a_b = (1. / (M_eff + pseudo_count)) * (pseudo_count/(q*q) + f_i_j_a_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the covariance matrix <a id=\"covmat\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_i_j_a_b.shape = (186, 20, 186, 20)\n"
     ]
    }
   ],
   "source": [
    "# Covariance Matrix\n",
    "# We take an outer product of f_i_a with itself using numpy's broadcasting rules. \n",
    "# This gives us a matrix where the (i,a, j, b) index is f[i,a] * f[j,b]\n",
    "C_i_j_a_b = f_i_j_a_b  - f_i_a[:, :, np.newaxis, np.newaxis] * f_i_a[np.newaxis, np.newaxis, :, :] \n",
    "\n",
    "# we project the covariance matrix down the first q-1 elements\n",
    "# Since the frequencies add up to 1 we can discard amino-acid value (a = q) for each site\n",
    "# without losing any information\n",
    "C_i_j_a_b = C_i_j_a_b[:, :(q-1), :, :(q-1)]\n",
    "print(\"C_i_j_a_b.shape = {}\".format(C_i_j_a_b.shape)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.9646153e-21, -6.6297967e-35, -4.1994404e-37, -4.1363380e-38,\n",
       "       -1.8372929e-38, -7.0018708e-39, -9.7111385e-41, -7.3652247e-42,\n",
       "       -0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.5687536e-41,\n",
       "        1.0106711e-39,  1.4769055e-38,  3.3672347e-38,  4.3045047e-38,\n",
       "        2.7075972e-37,  4.6586812e-21,  2.2118905e-10,  4.5262288e-05],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C00 = C_i_j_a_b[1, :, 1, :]\n",
    "np.linalg.eigvalsh(C00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Entropy Modeling and Direct Coupling Analysis<a id=\"maxent\"></a>\n",
    "\n",
    "It is hard to infer contact information from this covariance matrix as a non-zero covariance between two sites does not imply that they directly interact or are in contact. If we have site $i$ interacting  with $j$ and site $j$ interacting with $k$, then $i$ and $k$ will show correlations even though they do not directly interact. \n",
    "\n",
    "So we decide to look for a (protein family specific) probability model $\\bf P$ on the level of sequences that satisfies three conditions. \n",
    "1. The single site marginals of $\\bf P$ are equal to $f_i(a)$.   \n",
    "In other words, the sum of probabilities of all proteins with a in the $i$-th position is equal to the value of $f_i(a)$.\n",
    "2. The double site marginals of $\\bf P$ are equal to $f_{ij}(a,b)$.  \n",
    "In other words, the sum of probabilities of all proteins with $a$ in the $i$-th position and $b$ in the $j$-th position is equal to $f_{ij}(a,b)$.\n",
    "3. $\\bf P$ has the **maximum possible entropy** so we maximize\n",
    "$$ H[{\\bf P}] = - \\sum_{\\text{all sequences of length L}} P \\log P$$\n",
    "\n",
    "Condition 1 gives us $Lq$ constraints and Condition 2 gives us $\\frac{L(L+1)}{2} q^2$ additional constraints. This allows us to do a gigantic constrained optimization using Lagrange Multipliers and come up with an analytic form for $\\bf P$.\n",
    "$$ P(a_1, \\ldots, a_L) = \\frac{1}{Z} \\exp{ \\bigg\\{ \\frac{1}{2} \\sum_{i,j} e_{ij}(a_i, a_j) + \\sum_{i} h_i(a_i) \\bigg\\} }$$\n",
    "\n",
    "Here $Z$ is a normalization constant that makes sure that all the probabilities add up to $1$. The parameters $e_{ij}(a,b)$ are the direct couplings and $h_i(a)$ are the local fields acting on single sites. These values have to be determined so that Conditions 1 and 2 are satisfied. \n",
    "\n",
    "It is not possible to compute these parameters exactly for reasonably sized proteins as calculating $Z$ and the marginals require summations over all $q^L$ possible amino-acid sequences. \n",
    "\n",
    "**Mean Field Approximation** This is an approximation in statistical physics to help solve this intractable problem. We expand the exponential of the sum of the direct couplings using a Taylor Series expansion and then only keep the linear terms. With this approximation the values for the direct couplings are $$e_{ij}(a,b) = (C^{-1})_{ij}(a,b) \\qquad \\forall i,j, \\enspace \\forall a,b=1, \\ldots, q-1$$ and $$e_{ij}(a,q) = e_{ij}(q,a) = 0 \\qquad \\forall a = 1, \\ldots, q.$$\n",
    "This approximation approach is known as **direct coupling analysis**. Once the direct couplings have been calculated, they can be used to predict contacts between residues. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
