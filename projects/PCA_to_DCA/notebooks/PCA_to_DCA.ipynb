{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Principal Component (PCA) to Direct Coupling Analysis (DCA) of Coevolution in Proteins\n",
    "\n",
    "This notebook takes a look at a 2013 paper from Simona Cocco, Remi Monasson, Martin Weigt titled\n",
    "**From Principal Component to Direct Coupling Analysis of Coevolution in Proteins: Low-Eigenvalue Modes are Needed for Structure Prediction.** *\\[2013Cocco\\]*\n",
    "\n",
    "Link: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003176\n",
    "\n",
    "This paper looks at extracting functional and structural information from Multiple Sequence Alignments (MSA) of homologous proteins. First a covariance matrix of the residues are created from the MSA. Then the paper connects two approaches \n",
    "\n",
    "*  PCA - which identifies correlated groups of residues\n",
    "*  DCA - which identifies residue-residue contacts\n",
    "\n",
    "It shows how these two methods are related in non-intuitive ways using sophisticated statistical-physics models. This connection between the two approaches allows one to perform some sort of \"dimension reduction\" on DCA and to accurately predict residue-residue contacts with a smaller number of parameters. It also shows that the low eigenvalue values, which are discarded by PCA, are actually important to recover contact information. \n",
    "\n",
    "### Sections\n",
    "\n",
    "1. [Multiple Sequence Alignment](#msa)\n",
    "2. [Re-weighting Sequences](#reweight)\n",
    "3. [Compute Single and Double Site marginals](#marginals)\n",
    "4. [Compute the Covariance matrix](#covmat)\n",
    "5. [Maximum Entropy Modeling and Direct Coupling Analysis](#maxent)  \n",
    "6. [Approximations to DCA](#approxdca)\n",
    "7. [PSICOV - Sparse inverse covariance estimation](#2011PSICOV)\n",
    "8. [Correlation Matrix and connection to PCA](#pcaconnect)\n",
    "9. [What results does the paper 2013Cocco have?](#2013CoccoResults)\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Sequence Alignment (MSA)  <a id=\"msa\"></a>\n",
    "\n",
    "We use a [DHFR](https://www.uniprot.org/uniprot/P00374) alignment as an example to try out the methods of the papers. The alignment file is generated by [jackhmmer](https://www.ebi.ac.uk/Tools/hmmer/search/jackhmmer)\n",
    "\n",
    "\n",
    "```\n",
    "sp|P00374|DYR_HUMAN Dihydrofolate reductase OS=Homo sapiens OX=9606 GN=DHFR PE=1 SV=2\n",
    "MVGSLNCIVAVSQNMGIGKNGDLPWPPLRNEFRYFQRMTTTSSVEGKQNLVIMGKKTWFS\n",
    "IPEKNRPLKGRINLVLSRELKEPPQGAHFLSRSLDDALKLTEQPELANKVDMVWIVGGSS\n",
    "VYKEAMNHPGHLKLFVTRIMQDFESDTFFPEIDLEKYKLLPEYPGVLSDVQEEKGIKYKF\n",
    "EVYEKND\n",
    "```\n",
    "\n",
    "Note: Not exactly sure how the alignment is generated. \n",
    "\n",
    "```shell\n",
    "jackhmmer -A DHFR_uniref90.aln --noali --notextw P00374.fasta uniref90.fasta\n",
    "# some filtering script\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.linalg\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape = (56165, 186) ,dtype=  |S1\n"
     ]
    }
   ],
   "source": [
    "datadir = \"../data\"\n",
    "msa_file = os.path.join(datadir, \"DHFR.aln\")\n",
    "\n",
    "# Read all the lines in the file into a 2D array of type S1\n",
    "with open(msa_file) as fh:\n",
    "    arr = np.array([[x for x in line.strip()] for line in fh], np.dtype(\"S1\"))\n",
    "\n",
    "print(\"shape =\", arr.shape, \",dtype= \", arr.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences : 56165 \n",
      "Sequence Length : 186\n"
     ]
    }
   ],
   "source": [
    "# M is the number of sequences\n",
    "# L is the length\n",
    "M, L = arr.shape\n",
    "print(\"Number of sequences : {} \".format(M))\n",
    "print(\"Sequence Length : {}\".format(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'VRPLNCIVAVSQNMGIGKNGDLPWPPLRNEFKYFQRMTTTSSVEGKQNLVIMGRKTWFSIPEKNRPLKDRINIVLSRELKEPPRGAHFLAKSLDDALRLIEQPELASKVDMVWIVGGSSVYQEAMNQPGHLRLFVTRIMQEFESDTFFPEIDLGKYKLLPEYPGVLSEVQEEKGIKYKFEVYEKKD'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first sequence\n",
    "arr[0, :].tostring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'----SIVVVMCKRFGIGRNGVLPWSPLQADMQRFRSITAG-------GGVIMGRTTFDSIPEEHRPLQGRLNVVLTTSADLMKNSNIIFVSSFDELDAIVGL----HDHLPWHVIGGVSVYQHFLEKSQVTSMYVTFVDGSLECDTFFPHQFLSHFEITRA---SALMSDTTSGMSYRFVDYTR--'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the second sequence\n",
    "arr[1, :].tostring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can order the amino acids any way we like. Here is a sorting based on some amino acid properties. \n",
    "https://proteinstructures.com/Structure/Structure/amino-acids.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMINO_ACIDS = np.array([aa for aa in \"RKDEQNHSTCYWAILMFVPG-\"], \"S1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the weights of each sequence <a id=\"reweight\"></a>\n",
    "\n",
    "To compute the weight of a sequence, we first compute the hamming distance between this sequence and all the other sequences in the alignment. Then we count the number of these distances that are less than a cutoff. \n",
    "\n",
    "This count is 1 for isolated sequences. It is large for sequences that have many similar sequences in the alignment. The weight is reciprocal of the count. So it is 1 for isolated sequences and close to zero for sequences that have many similar sequences in the MSA. (Eqn 27 in 2013Cocco)\n",
    "\n",
    "$$w_m = \\frac{1}{ \\| \\{ n | 1 \\leq n \\leq M  ; d_H [ (a_1^n, \\ldots, a_L^n), (a_1^m, \\ldots, a_L^m) ] \\leq xL \\} \\| }$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005681818181818182"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamming_cutoff = 0.2 # This is x in the equation above\n",
    "def compute_weight(index, x=hamming_cutoff, arr=arr):\n",
    "    hamming_distances = np.sum(arr[index, :] != arr, axis=1)\n",
    "    count = np.sum(hamming_distances <= x * arr.shape[1]) # L = arr.shape[1]\n",
    "    return (1.0 / count)\n",
    "\n",
    "# compute the weight of the first sequence\n",
    "compute_weight(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from :  ../data/DHFR.weights.npy\n"
     ]
    }
   ],
   "source": [
    "progress_bar = True\n",
    "try:\n",
    "    from IPython.display import clear_output\n",
    "except ImportError:\n",
    "     progress_bar = False\n",
    "\n",
    "weights_file = os.path.join(datadir, \"DHFR.weights.npy\")\n",
    "\n",
    "if os.path.isfile(weights_file):\n",
    "    weights = np.load(weights_file)\n",
    "    print(\"Loading weights from : \", weights_file)\n",
    "\n",
    "else:\n",
    "    weights = np.zeros(M, dtype=np.float64)\n",
    "\n",
    "    for i in range(M):\n",
    "        weights[i] = compute_weight(i)\n",
    "        if i % 100 == 0:\n",
    "            if progress_bar:\n",
    "                clear_output(wait=True)\n",
    "            print (\"Processing sequence\", i, \"of\", M)\n",
    "    np.save(weights_file, weights)\n",
    "    print(\"Finished computing sequence weights and saved to : \", weights_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15238\n"
     ]
    }
   ],
   "source": [
    "# number of effective sequences\n",
    "M_eff = sum(weights) # Eqn 28 in 2013Cocco\n",
    "print(int(round(M_eff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q is the alphabet\n",
    "q = len(AMINO_ACIDS)\n",
    "pseudo_count = round(M_eff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Weighted Single and Double site marginals <a id=\"marginals\"></a>\n",
    "\n",
    "We first compute the weighted counts for the single site marginals $$\\sum_{m=1}^M w_m \\delta_{a, a_i^m},$$ and then we compute the weighted counts for the double site marginals $$\\sum_{m=1}^M w_m \\delta_{a, a_i^m} \\delta_{b, a_j^m}.$$ The second computation takes a long time since the arrays are too large to compute outer products and broadcast them. \n",
    "\n",
    "To get the actual marignals we need to divide these weighted counts above by the sum of the weights. However, we only do this after adding a pseudocount to regularize the resulting covariance matrix. (Eqns 29 and 30 in 2013Cocco.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading single site marginals from  ../data/DHFR.single.npy\n",
      "Loading double site marginals from  ../data/DHFR.double.npy\n"
     ]
    }
   ],
   "source": [
    "single_site_marginal_file = os.path.join(datadir, \"DHFR.single.npy\")\n",
    "double_site_marginal_file = os.path.join(datadir, \"DHFR.double.npy\")\n",
    "\n",
    "if os.path.isfile(double_site_marginal_file) and os.path.isfile(single_site_marginal_file):\n",
    "    f_i_a = np.load(single_site_marginal_file)\n",
    "    print(\"Loading single site marginals from \", single_site_marginal_file)\n",
    "\n",
    "    f_i_j_a_b = np.load(double_site_marginal_file)\n",
    "    print(\"Loading double site marginals from \", double_site_marginal_file)    \n",
    "\n",
    "else:\n",
    "    # We first compute a one-hot matrix of shape (M, L, q)\n",
    "    # which is 0/1 in index (m, i, a) depending on whether \n",
    "    # Protein *m* has amino acid *a* in position *i* \n",
    "    arr_onehot = np.zeros(arr.shape + (q,), dtype=np.uint8)\n",
    "    for i, a in enumerate(AMINO_ACIDS):\n",
    "        arr_onehot[..., i] = (arr == a)\n",
    "    print(\"arr_onehot.shape = {}\".format(arr_onehot.shape))\n",
    "    \n",
    "    # we reorder the one-hot axes so that the sequences are in the last dimension\n",
    "    # this allows us to multiply easily by the weights using broadcasting\n",
    "    arr_onehot_reorder = np.moveaxis(arr_onehot, 0, 2)\n",
    "    weighted_arr_onehot = arr_onehot_reorder * weights\n",
    "\n",
    "    \n",
    "    # Set up the weighted double site marginals array\n",
    "    f_i_j_a_b = np.zeros((L, q, L, q), dtype=weights.dtype)\n",
    "    \n",
    "    # we cannot use outer products here because our arrays are too big\n",
    "    # So we iterate\n",
    "    for j, b in itertools.product(range(L), range(q)):\n",
    "        f_i_j_a_b[:, :, j, b] = np.sum((weighted_arr_onehot * arr_onehot_reorder[j, b, :]), axis=-1)\n",
    "        if progress_bar:\n",
    "            clear_output(wait=True)\n",
    "        print(\"Finished processing j={}, b={}, AA={}\".format(j, b, AMINO_ACIDS[b].tostring().decode()))\n",
    "\n",
    "    # Weighted Single Site Marignals\n",
    "    f_i_a = np.sum((arr_onehot_reorder * weights), axis=-1)\n",
    "    print(\"f_i_a.shape = {}\".format(f_i_a.shape)) \n",
    "\n",
    "    \n",
    "    np.save(single_site_marginal_file, f_i_a)\n",
    "    np.save(double_site_marginal_file, f_i_j_a_b)\n",
    "    \n",
    "    # delete large temporary arrays\n",
    "    del weighted_arr_onehot, arr_onehot_reorder, arr_onehot\n",
    "    print(\"Finished computing single and double site marginals and saved to cache files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Pseudo count and compute the marginals (Eqn 29 and 30 2013Cocco)\n",
    "\n",
    "pseudo_count_ratio = float(pseudo_count) / (M_eff + pseudo_count)\n",
    "f_i_a = (pseudo_count_ratio / q ) + (1 - pseudo_count_ratio) * f_i_a / M_eff\n",
    "f_i_j_a_b = (pseudo_count_ratio  / (q*q) ) + (1 - pseudo_count_ratio) * f_i_j_a_b / M_eff\n",
    "\n",
    "\n",
    "site_identity_mask = np.identity(q, dtype=bool)\n",
    "for i in range(L):\n",
    "    f_i_j_a_b[i, :, i, :][~site_identity_mask] = 0.\n",
    "    f_i_j_a_b[i, :, i, :][site_identity_mask] += (pseudo_count_ratio / q) - (pseudo_count_ratio / (q*q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the covariance matrix <a id=\"covmat\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_i_j_a_b.shape = (186, 20, 186, 20)\n"
     ]
    }
   ],
   "source": [
    "# Covariance Matrix\n",
    "# We take an outer product of f_i_a with itself using numpy's broadcasting rules. \n",
    "# This gives us a matrix where the (i,a, j, b) index is f[i,a] * f[j,b]\n",
    "C_i_j_a_b = f_i_j_a_b  - f_i_a[:, :, np.newaxis, np.newaxis] * f_i_a[np.newaxis, np.newaxis, :, :] \n",
    "\n",
    "# we project the covariance matrix down the first q-1 elements\n",
    "# Since the frequencies add up to 1 we can discard amino-acid value (a = q) for each site\n",
    "# without losing any information\n",
    "C_i_j_a_b = C_i_j_a_b[:, :(q-1), :, :(q-1)]\n",
    "print(\"C_i_j_a_b.shape = {}\".format(C_i_j_a_b.shape)) \n",
    "\n",
    "del f_i_a, f_i_j_a_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Entropy Modeling and Direct Coupling Analysis<a id=\"maxent\"></a>\n",
    "\n",
    "It is hard to infer contact information from this covariance matrix as a non-zero covariance between two sites does not imply that they directly interact or are in contact. If we have site $i$ interacting  with $j$ and site $j$ interacting with $k$, then $i$ and $k$ will show correlations even though they do not directly interact. \n",
    "\n",
    "So we decide to look for a (protein family specific) probability model $\\bf P$ on the level of sequences that satisfies three conditions. \n",
    "1. The single site marginals of $\\bf P$ are equal to $f_i(a)$.   \n",
    "In other words, the sum of probabilities of all proteins with a in the $i$-th position is equal to the value of $f_i(a)$.\n",
    "2. The double site marginals of $\\bf P$ are equal to $f_{ij}(a,b)$.  \n",
    "In other words, the sum of probabilities of all proteins with $a$ in the $i$-th position and $b$ in the $j$-th position is equal to $f_{ij}(a,b)$.\n",
    "3. $\\bf P$ has the **maximum possible entropy** so we maximize\n",
    "$$ H[{\\bf P}] = - \\sum_{\\text{all sequences of length L}} P \\log P$$\n",
    "\n",
    "Condition 1 gives us $Lq$ constraints and Condition 2 gives us $\\frac{L(L-1)}{2} q^2$ additional constraints. This allows us to do a gigantic constrained optimization using Lagrange Multipliers and come up with an analytic form for $\\bf P$.\n",
    "$$ P(a_1, \\ldots, a_L) = \\frac{1}{Z} \\exp{ \\bigg\\{ \\frac{1}{2} \\sum_{i,j} e_{ij}(a_i, a_j) + \\sum_{i} h_i(a_i) \\bigg\\} }$$\n",
    "\n",
    "Here $Z$ is a normalization constant that makes sure that all the probabilities add up to $1$. The parameters $e_{ij}(a,b)$ are the direct couplings and $h_i(a)$ are the local fields acting on single sites. These values have to be determined so that Conditions 1 and 2 are satisfied. \n",
    "\n",
    "It is not possible to compute these parameters exactly for reasonably sized proteins as calculating $Z$ and the marginals require summations over all $q^L$ possible amino-acid sequences. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximations to DCA <a id=\"dcaapprox\"></a>\n",
    "There are [several approximations](https://en.wikipedia.org/wiki/Direct_coupling_analysis) to estimate the parameters above\n",
    "\n",
    "* Boltzmann Machine Learning (BM)\n",
    "* Message passing / belief propagation (mpDCA)\n",
    "* Mean Field Approximation (mfDCA)\n",
    "* Gaussian Approximation (gaussDCA)\n",
    "* Pseudolikelihoods Maximization (PLM)\n",
    "* Adaptive Cluster Expansion (ACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Field Approximation <a id=\"meanfield\"></a>\n",
    "This is an approximation in statistical physics to help solve this intractable problem. We expand the exponential of the sum of the direct couplings using a Taylor Series expansion and then only keep the linear terms. With this approximation the values for the direct couplings are $$e_{ij}(a,b) = (C^{-1})_{ij}(a,b) \\qquad \\forall i,j, \\enspace \\forall a,b=1, \\ldots, q-1$$ and $$e_{ij}(a,q) = e_{ij}(q,a) = 0 \\qquad \\forall a = 1, \\ldots, q,$$ and $$ h_i(q) = 0.$$\n",
    "\n",
    "This approximation approach is known as **mean field direct coupling analysis (mfDCA)**. Once the direct couplings have been calculated, they can be used to predict contacts between residues. This post processing prediction step will be covered later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of eigenvalues: 3720\n",
      "Number of eigenvalues non-positive: 0\n"
     ]
    }
   ],
   "source": [
    "# Our Covariance matrix isn't invertible\n",
    "cov = C_i_j_a_b.reshape((L*(q-1), L*(q-1)))\n",
    "eigvals = np.linalg.eigvalsh(cov)\n",
    "\n",
    "print(\"Number of eigenvalues: {}\".format(eigvals.size))\n",
    "print(\"Number of eigenvalues non-positive: {}\".format(np.sum(eigvals <= 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSICOV - Sparse inverse covariance estimation <a id=\"2011PSICOV\"></a>\n",
    "\n",
    "The 2011 paper *PSICOV - precise structural contact prediction using sparse inverse covariance estimation* (**2011PSICOV**) deals with extracting information directly by inverting the unprojected covariance matrix $C_{ij}(a, b)$. \n",
    "\n",
    "This inversion gives a matrix $\\Theta$ of partial correlations (i.e correlation between two sites controlling for the effect of all other sites). The covariance matrix is singular and so **2011PSICOV** regularizes the matrix in addition to asking for sparse solutions for the inverse via LASSO. \n",
    "\n",
    "This matrix $\\Theta$ is used to predict contacts in the following way. The L1 norm of the submatrix of $\\Theta$ which consists of 20x20 amino acids (the gap is ignored)  at i and j is calculated. \n",
    "$$S_{ij}^{\\text{contact}} = \\sum_{ab} |\\Theta_{ij}(a,b)|.$$\n",
    "\n",
    "This score is then corrected for averages across sites called an *Average Product correction* (APC) to give a final PSICOV score for pairs of sites $i$ and $j$. $$PC_{ij} = S_{ij}^{\\text{contact}} - \\frac{\\bar{S}_{i-}^{\\text{contact}} \\bar{S}_{-j}^{\\text{contact}}}{\\bar{S}_{--}^{\\text{contact}}}$$\n",
    "\n",
    "Also, this somewhat coincides with the mean field approximation under DCA. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix and connection to PCA <a id=\"pcaconnect\"></a>\n",
    "\n",
    "We construct the Pearson Correlation Matrix $\\Gamma$ through the relationship $$\\Gamma_{ij}(a,b) = \\sum_{c,d = 1}^{q-1} (D_i)^{-1} (a, c) C_{ij}(c,d) (D_j)^{-1}(d,b),$$ where the matrices $D_i$ are the square roots of the single-site correlation matrices, i.e. $$C_{ii}(a, b) = \\sum_{c=1}^{q-1} D_i(a,c)D_i(c, b).$$\n",
    "\n",
    "$\\Gamma$ is normalized and on each site is a $(q-1) \\times (q-1)$ identity matrix. $$\\Gamma_{ij}(a,b) = \\delta_{ab}.$$\n",
    "\n",
    "#### Eigenvalues and Eigenvectors <a id=\"eigen\"></a>\n",
    "Once we have the correlation matrix we can find the eigenvalues and the eigenvectors. $\\mu = 1, \\ldots, L(q-1)$\n",
    "\n",
    "$$\\sum_{j=1}^L \\sum_{b=1}^{q-1} \\Gamma_{ij}(a,b) v^{\\mu}_{jb} = \\lambda_{\\mu} v^{\\mu}_{ia},$$ where the eigenvalues are ordered in decreasing order and the eigenvectors are chosen to form an ortho-normal basis. Also, only the top eigenvalues are chosen and all other eigenvalues are discarded. \n",
    "\n",
    "#### Other places PCA has been used <a id=\"otherpca\"></a>\n",
    "PCA has been used on MSA data to identity functional sites as well as sectors (culsters of evolutionarily correlated sites). \n",
    "\n",
    "**Specificity-determing positions** PCA was used by Casari et al. to identity functional sites specific to subfamilies. Here, PCA was used on a Comparison matrix $C(m, m')$ which counts the number of identical resides between each pair of sequences $(m, m'=1, \\ldots, M)$. This is a very different matrix from $\\Gamma$.\n",
    "\n",
    "**Statistical Coupling Analysis** This approach was used by Ranganathan et al. Here, a modified residue covariance matrix is defined : $$\\tilde{C}^{SCA}_{ij}(a,b) = w_i^a C_{ij}(a,b) w_j^b,$$ where the weights $w_i^a$ favor positions $i$ and resides $a$ of high conservation. Then an effective covariance matrix is defined between positions only $$\\tilde{C}^{SCA}_{ij} = \\sqrt{\\sum_{a,b} \\tilde{C}^{SCA}(a,b)^2}.$$\n",
    "\n",
    "Principal component analysis is then applied to the $L$-dimensional $\\tilde{C}^{SCA}$ matrix and used to define sectors (i.e. clusters of evolutionaily correlated sites)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_ii = np.zeros_like(C_i_j_a_b)\n",
    "for i in range(L):\n",
    "    C_ii[i, :, i, :] = C_i_j_a_b[i, :, i, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3720, 3720)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.linalg\n",
    "\n",
    "Dii = scipy.linalg.sqrtm(C_ii.reshape(L*(q-1), L*(q-1)))\n",
    "\n",
    "Dii.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dii_inv = np.linalg.inv(Dii)\n",
    "\n",
    "Gij = Dii_inv @ cov @ Dii_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What results does the paper 2013Cocco have? <a id=\"2013CoccoResults\"></a>\n",
    "\n",
    "The coupling matrix $e_{ij}(a,b)$ has dimension $L(q-1) \\times L(q-1)$ and the paper wants to find low rank versions of this coupling matrix which offers a way to reduce the number of parameters much below that of the mean field approximation. Also the solutions that they get are connected to the eigenvalues and eigenvectors of the correlation matrix $\\Gamma$. This establishes a connection between DCA and PCA. The connection is a little counter-intuitive because the low eigenvalues of $\\Gamma$ are actually important for determining the the low rank coupling matrix. \n",
    "\n",
    "It does not look like this connection has much to do with SCA or finding sectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evals shape (3720,)\n",
      "Evals max : 83.66865152743992\n",
      "Evals min : 0.09090944380745972\n"
     ]
    }
   ],
   "source": [
    "evals = np.linalg.eigvalsh(Gij)\n",
    "print(\"Evals shape {}\".format(evals.shape))\n",
    "print(\"Evals max : {}\".format(evals.max()))\n",
    "print(\"Evals min : {}\".format(evals.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "_ = plt.hist(evals, bins=500)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
