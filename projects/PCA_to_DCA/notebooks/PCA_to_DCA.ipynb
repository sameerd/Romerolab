{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Principal Component (PCA) to Direct Coupling Analysis (DCA) of Coevolution in Proteins\n",
    "\n",
    "This notebook takes a look at a 2013 paper from Simona Cocco, Remi Monasson, Martin Weigt titled\n",
    "**From Principal Component to Direct Coupling Analysis of Coevolution in Proteins: Low-Eigenvalue Modes are Needed for Structure Prediction.** *\\[2013Cocco\\]*\n",
    "\n",
    "Link: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003176\n",
    "\n",
    "This paper looks at extracting functional and structural information from Multiple Sequence Alignments (MSA) of homologous proteins. First a covariance matrix of the residues are created from the MSA. Then the paper connects two approaches \n",
    "\n",
    "*  PCA - which identifies correlated groups of residues\n",
    "*  DCA - which identifies residue-residue contacts\n",
    "\n",
    "It shows how these two methods are related in non-intuitive ways using sophisticated statistical-physics models. This connection between the two approaches allows one to perform some sort of \"dimension reduction\" on DCA and to accurately predict residue-residue contacts with a smaller number of parameters. It also shows that the low eigenvalue values, which are discarded by PCA, are actually important to recover contact information. \n",
    "\n",
    "### Sections\n",
    "\n",
    "1. [Multiple Sequence Alignment](#msa)\n",
    "2. [Re-weighting Sequences](#reweight)\n",
    "3. [Compute Single and Double Site marginals](#marginals)\n",
    "4. [Compute the Covariance matrix](#covmat)\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Sequence Alignment (MSA)  <a id=\"msa\"></a>\n",
    "\n",
    "We use a [DHFR](https://www.uniprot.org/uniprot/P00374) alignment as an example to try out the methods of the papers. The alignment file is generated by [jackhmmer](https://www.ebi.ac.uk/Tools/hmmer/search/jackhmmer)\n",
    "\n",
    "\n",
    "```\n",
    "sp|P00374|DYR_HUMAN Dihydrofolate reductase OS=Homo sapiens OX=9606 GN=DHFR PE=1 SV=2\n",
    "MVGSLNCIVAVSQNMGIGKNGDLPWPPLRNEFRYFQRMTTTSSVEGKQNLVIMGKKTWFS\n",
    "IPEKNRPLKGRINLVLSRELKEPPQGAHFLSRSLDDALKLTEQPELANKVDMVWIVGGSS\n",
    "VYKEAMNHPGHLKLFVTRIMQDFESDTFFPEIDLEKYKLLPEYPGVLSDVQEEKGIKYKF\n",
    "EVYEKND\n",
    "```\n",
    "\n",
    "Note: Not exactly sure how the alignment is generated. \n",
    "\n",
    "```shell\n",
    "jackhmmer -A DHFR_uniref90.aln --noali --notextw P00374.fasta uniref90.fasta\n",
    "# some filtering script\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape = (56165, 186) ,dtype=  |S1\n"
     ]
    }
   ],
   "source": [
    "datadir = \"../data\"\n",
    "msa_file = os.path.join(datadir, \"DHFR.aln\")\n",
    "\n",
    "# Read all the lines in the file into a 2D array of type S1\n",
    "with open(msa_file) as fh:\n",
    "    arr = np.array([[x for x in line.strip()] for line in fh], np.dtype(\"S1\"))\n",
    "\n",
    "print(\"shape =\", arr.shape, \",dtype= \", arr.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences : 56165 \n",
      "Sequence Length : 186\n"
     ]
    }
   ],
   "source": [
    "# M is the number of sequences\n",
    "# L is the length\n",
    "M, L = arr.shape\n",
    "print(\"Number of sequences : {} \".format(M))\n",
    "print(\"Sequence Length : {}\".format(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'VRPLNCIVAVSQNMGIGKNGDLPWPPLRNEFKYFQRMTTTSSVEGKQNLVIMGRKTWFSIPEKNRPLKDRINIVLSRELKEPPRGAHFLAKSLDDALRLIEQPELASKVDMVWIVGGSSVYQEAMNQPGHLRLFVTRIMQEFESDTFFPEIDLGKYKLLPEYPGVLSEVQEEKGIKYKFEVYEKKD'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first sequence\n",
    "arr[0, :].tostring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'----SIVVVMCKRFGIGRNGVLPWSPLQADMQRFRSITAG-------GGVIMGRTTFDSIPEEHRPLQGRLNVVLTTSADLMKNSNIIFVSSFDELDAIVGL----HDHLPWHVIGGVSVYQHFLEKSQVTSMYVTFVDGSLECDTFFPHQFLSHFEITRA---SALMSDTTSGMSYRFVDYTR--'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the second sequence\n",
    "arr[1, :].tostring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can order the amino acids any way we like. Here is a sorting based on some amino acid properties. \n",
    "https://proteinstructures.com/Structure/Structure/amino-acids.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMINO_ACIDS = np.array([aa for aa in \"RKDEQNHSTCYWAILMFVPG-\"], \"S1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the weights of each sequence <a id=\"reweight\"></a>\n",
    "\n",
    "To compute the weight of a sequence, we first compute the hamming distance between this sequence and all the other sequences in the alignment. Then we count the number of these distances that are less than a cutoff. \n",
    "\n",
    "This count is 1 for isolated sequences. It is large for sequences that have many similar sequences in the alignment. The weight is reciprocal of the count. So it is 1 for isolated sequences and close to zero for sequences that have many similar sequences in the MSA. (Eqn 27 in 2013Cocco)\n",
    "\n",
    "$$w_m = \\frac{1}{ \\| \\{ n | 1 \\leq n \\leq M  ; d_H [ (a_1^n, \\ldots, a_L^n), (a_1^m, \\ldots, a_L^m) ] \\leq xL \\} \\| }$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005681818181818182"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamming_cutoff = 0.2 # This is x in the equation above\n",
    "def compute_weight(index, x=hamming_cutoff, arr=arr):\n",
    "    hamming_distances = np.sum(arr[index, :] != arr, axis=1)\n",
    "    count = np.sum(hamming_distances <= x * arr.shape[1]) # L = arr.shape[1]\n",
    "    return (1.0 / count)\n",
    "\n",
    "# compute the weight of the first sequence\n",
    "compute_weight(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from :  ../data/DHFR.weights.npy\n"
     ]
    }
   ],
   "source": [
    "progress_bar = True\n",
    "try:\n",
    "    from IPython.display import clear_output\n",
    "except ImportError:\n",
    "     progress_bar = False\n",
    "\n",
    "weights_file = os.path.join(datadir, \"DHFR.weights.npy\")\n",
    "\n",
    "if os.path.isfile(weights_file):\n",
    "    weights = np.load(weights_file)\n",
    "    print(\"Loading weights from : \", weights_file)\n",
    "\n",
    "else:\n",
    "    weights = np.zeros(M)\n",
    "\n",
    "    for i in range(M):\n",
    "        weights[i] = compute_weight(i)\n",
    "        if i % 100 == 0:\n",
    "            if progress_bar:\n",
    "                clear_output(wait=True)\n",
    "            print (\"Processing sequence\", i, \"of\", M)\n",
    "    np.save(weights_file, weights)\n",
    "    print(\"Finished computing sequence weights and saved to : \", weights_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15238\n"
     ]
    }
   ],
   "source": [
    "# number of effective sequences\n",
    "M_eff = sum(weights) # Eqn 28 in 2013Cocco\n",
    "print(int(round(M_eff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q is the alphabet\n",
    "q = len(AMINO_ACIDS)\n",
    "pseudo_count = round(M_eff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Weighted Single and Double site marginals <a id=\"marginals\"></a>\n",
    "\n",
    "We first compute the weighted counts $$\\sum_{m=1}^M w_m \\delta_{a, a_i^m}$$ and then $$\\sum_{m=1}^M w_m \\delta_{a, a_i^m} \\delta_{b, a_j^m}$$ and save them to disk. The second computation takes a long time since the arrays are too large to compute outer products and broadcast them. \n",
    "\n",
    "To get the marignals we need to divide these counts above by the sum of the weights. However, we only do this after adding a pseudocount in Eqns 29 and 30 in 2013Cocco.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading single site marginals from  ../data/DHFR.single.npy\n",
      "Loading double site marginals from  ../data/DHFR.double.npy\n"
     ]
    }
   ],
   "source": [
    "single_site_marginal_file = os.path.join(datadir, \"DHFR.single.npy\")\n",
    "double_site_marginal_file = os.path.join(datadir, \"DHFR.double.npy\")\n",
    "\n",
    "if os.path.isfile(double_site_marginal_file) and os.path.isfile(single_site_marginal_file):\n",
    "    f_i_a = np.load(single_site_marginal_file)\n",
    "    print(\"Loading single site marginals from \", single_site_marginal_file)\n",
    "\n",
    "    f_i_j_a_b = np.load(double_site_marginal_file)\n",
    "    print(\"Loading double site marginals from \", double_site_marginal_file)    \n",
    "\n",
    "else:\n",
    "    # We first compute a one-hot matrix of shape (M, L, q)\n",
    "    # which is 0/1 in index (m, i, a) depending on whether \n",
    "    # Protein *m* has amino acid *a* in position *i* \n",
    "    arr_onehot = np.zeros(arr.shape + (q,), dtype=np.uint8)\n",
    "    for i, a in enumerate(AMINO_ACIDS):\n",
    "        arr_onehot[..., i] = (arr == a)\n",
    "    print(\"arr_onehot.shape = {}\".format(arr_onehot.shape))\n",
    "    \n",
    "    # we reorder the one-hot axes so that the sequences are in the last dimension\n",
    "    # this allows us to multiply easily by the weights using broadcasting\n",
    "    arr_onehot_reorder = np.moveaxis(arr_onehot, 0, 2)\n",
    "    weighted_arr_onehot = arr_onehot_reorder * weights\n",
    "\n",
    "    # Weighted Single Site Marignals\n",
    "    f_i_a = np.sum((arr_onehot_reorder * weights), axis=-1)\n",
    "    print(\"f_i_a.shape = {}\".format(f_i_a.shape)) \n",
    "    \n",
    "    # Set up the weighted double site marginals array\n",
    "    f_i_j_a_b = np.zeros((L, q, L, q), dtype=f_i_a.dtype)\n",
    "    \n",
    "    # we cannot use outer products here because our arrays are too big\n",
    "    # So we iterate\n",
    "    for j, b in itertools.product(range(L), range(q)):\n",
    "        f_i_j_a_b[:, :, j, b] = np.sum((weighted_arr_onehot * arr_onehot_reorder[j, b, :]), axis=-1)\n",
    "    if progress_bar:\n",
    "        clear_output(wait=True)\n",
    "    print(\"Finished processing j={}, b={}, AA={}\".format(j, b, AMINO_ACIDS[b].tostring().decode()))\n",
    "\n",
    "    np.save(single_site_marginal_file, f_i_a)\n",
    "    np.save(double_site_marginal_file, f_i_j_a_b)\n",
    "    \n",
    "    # delete large temporary arrays\n",
    "    del weighted_arr_onehot, arr_onehot_reorder, arr_onehot\n",
    "    print(\"Finished computing single and double site marginals and saved to cache files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Pseudo count and compute the marginals (Eqn 29 and 30 2013Cocco)\n",
    "\n",
    "f_i_a = 1. / (M_eff + pseudo_count) * (pseudo_count/q + f_i_a)\n",
    "f_i_j_a_b = 1. / (M_eff + pseudo_count) * (pseudo_count/(q*q) + f_i_j_a_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the covariance matrix <a id=\"covmat\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_i_j_a_b.shape = (186, 20, 186, 20)\n"
     ]
    }
   ],
   "source": [
    "# Covariance Matrix\n",
    "# We take an outer product of f_i_a with itself using numpy's broadcasting rules. \n",
    "# This gives us a matrix where the (i,a, j, b) index is f[i,a] * f[j,b]\n",
    "C_i_j_a_b = f_i_j_a_b  - f_i_a[:, :, np.newaxis, np.newaxis] * f_i_a[np.newaxis, np.newaxis, :, :] \n",
    "\n",
    "# we project the covariance matrix down the first q-1 elements\n",
    "C_i_j_a_b = C_i_j_a_b[:, :(q-1), :, :(q-1)]\n",
    "print(\"C_i_j_a_b.shape = {}\".format(C_i_j_a_b.shape)) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
